#!/bin/sh
#
# ----IMPORTANT INFORMATION FOR RUNNING THIS SCRIPT----
# On WCOSS systems run BC creation for all hours simultaneously
# For Phase2/Phase3 systems this requires setting -n <nodes>
# to the number of BC files to be created. For the Cray systems
# set NODES to the number of BC files to be created.
#
# On Theia all other batch commands must be commented out. The
# BC files are created sequentially so make sure the queue/time
# is set properly.
#
#----WCOSS DELL JOBCARD
#BSUB /bin/sh
#BSUB -P FV3GFS-T2O
#BSUB -n 8
#BSUB -R span[ptile=1]
#BSUB -o log.chgres.for_BC_C768.%J
#BSUB -e log.chgres.for_BC_C768.%J
#BSUB -J grid_fv3
#BSUB -q debug
#BSUB -W 00:30
#----WCOSS_CRAY JOBCARD
##BSUB -L /bin/sh
##BSUB -P FV3GFS-T2O
##BSUB -oo log.chgres_forBC.%J
##BSUB -eo log.chgres_forBC.%J
##BSUB -J chgres_fv3
##BSUB -q debug
##BSUB -W 00:30
##BSUB -M 1024
##BSUB -extsched 'CRAYLINUX[]'
#----WCOSS JOBCARD
##BSUB -L /bin/sh
##BSUB -a poe
##BSUB -P FV3GFS-T2O
##BSUB -oo log.chgres_forBC.%J
##BSUB -eo log.chgres_forBC.%J
##BSUB -J chgres_fv3
##BSUB -q debug2
##BSUB -x
##BSUB -a openmp
##BSUB -n 4
##BSUB -R span[ptile=1]
#
#----THEIA SLURM JOBCARD
#SBATCH -N 1 --ntasks-per-node=24
#SBATCH -t 0:30:00
#SBATCH -A fv3-cam
#SBATCH -J chgres_fv3_bc
#SBATCH -q debug
#SBATCH -o log.chgres.forBC_C768.%j
#SBATCH -e log.chgres.forBC_C768.%j
#SBATCH -D.
set -x
#
#
# the following exports can all be set or just will default to what is in global_chgres_driver.sh
#
export gtype=regional        # grid type = uniform, stretch, nest, or regional
export OMP_NUM_THREADS_CH=24 #default for openMP threads
export machine=DELL         #WCOSS_C,WCOSS,THEIA
export CASE=C768              # resolution of tile: 48, 96, 192, 384, 768, 1152, 3072
export CDATE=2019041000      # format yyyymmddhh yyyymmddhh ...
export LEVS=64
export LSOIL=4
export NTRAC=7
export ictype=pfv3gfs        # opsgfs for q3fy17 gfs with new land datasets; oldgfs for q2fy16 gfs, pfv3gfs for parallel fv3 input
export nst_anl=.false.       # false or true to include NST analysis
export NSTANL=NULL
#
# NOTE: we have added ictype=pfv3gfs to allow for use of the FV3 parallel run data for chgres. In this job it is used to
#       define the location of the data on theia and luna/surge. The job runs global_chgres_driver.sh. That script sets
#       ictype=opsgf and then exports variables for ATM,SFC and NST. With FV3 input we no longer need the NST file as NSST data
#       is in the surface file. We have reached out to Fanglin to modify the script but for now it works fine, just adds
#       an unnecessary export.
#
if [ $machine = WCOSS_C ]; then
 export NODES=28
 . $MODULESHOME/init/sh 2>>/dev/null
 module load PrgEnv-intel prod_envir  cfp-intel-sandybridge/1.1.0 2>>/dev/null
 module list
 export BASE_GSM=/gpfs/hps3/emc/meso/noscrub/${LOGNAME}/fv3sar_workflow
 export KMP_AFFINITY=disabled
 export FIXgsm=/gpfs/hps3/emc/global/noscrub/emc.glopara/git/fv3gfs/fix/fix_am
 export APRUNC="aprun -n 1 -N 1 -j 1 -d $OMP_NUM_THREADS_CH -cc depth"
 if [ $ictype = pfv3gfs ]; then
  hour=`echo $CDATE | cut -c 9-10`
  export INIDIR=/gpfs/dell1/nco/ops/com/gfs/para/gfs.$ymd/$hour
 else
  export INIDIR=/gpfs/hps/nco/ops/com/gfs/prod/gfs.$ymd
 fi
 export HOMEgfs=$LS_SUBCWD/..
elif [ $machine = WCOSS ]; then
 . /usrx/local/Modules/default/init/sh 2>>/dev/null
 module load ics/12.1 ibmpe cfp NetCDF/4.2/serial 2>>/dev/null
 module list
 export APRUNC="time"
 export BASE_GSM=/gpfs/gd3/ibm/noscrub/${LOGNAME}/fv3sar_workflow
 export FIXgsm=/gpfs/hps3/emc/global/noscrub/emc.glopara/git/fv3gfs/fix/fix_am
 export DATA=/ptmpp2/${LOGNAME}/wrk.chgres
 export ymd=`echo $CDATE | cut -c 1-8`
 if [ $ictype = pfv3gfs ]; then
  hour=`echo $CDATE | cut -c 9-10`
  export INIDIR=/gpfs/dell1/nco/ops/com/gfs/para/gfs.$ymd/$hour
 else
  export INIDIR=/gpfs/hps/nco/ops/com/gfs/prod/gfs.$ymd
 fi
 export HOMEgfs=$LS_SUBCWD/..
elif [ $machine = DELL ]; then
 set +x
 . /usrx/local/prod/lmod/lmod/init/sh
 module load EnvVars/1.0.2 settarg/7.7 lsf/10.1 prod_envir/1.0.2
 module load ips/18.0.1.163 
 module load impi/18.0.1
 module load NetCDF/4.5.0
 module load HDF5-serial/1.10.1
 module load CFP/2.0.1
 module list
 set -x
 export APRUNC="time"
 export BASE_GSM=/gpfs/dell2/emc/modeling/noscrub/${LOGNAME}/fv3sar_workflow
 export FIXgsm=$BASE_GSM/fix/fix_am
 export KMP_AFFINITY=disabled
 export DATA=/gpfs/dell3/ptmp/${LOGNAME}/wrk.chgres
 export ymd=`echo $CDATE | cut -c 1-8`
 if [ $ictype = pfv3gfs ]; then
  hour=`echo $CDATE | cut -c 9-10`
  export INIDIR=/gpfs/dell1/nco/ops/com/gfs/para/gfs.$ymd/$hour
#  export INIDIR=/gpfs/dell3/ptmp/emc.glopara/ROTDIRS/prfv3rt1/gfs.$ymd/$hour
 else
  export INIDIR=/gpfs/hps/nco/ops/com/gfs/prod/gfs.$ymd
 fi
 export HOMEgfs=$LS_SUBCWD/..
elif [ $machine = THEIA ]; then
 . /apps/lmod/lmod/init/sh
 module use -a /scratch3/NCEPDEV/nwprod/lib/modulefiles
 module load intel/16.1.150 netcdf/4.3.0 hdf5/1.8.14 2>>/dev/null
 module list
 export BASE_GSM=/scratch4/NCEPDEV/fv3-cam/noscrub/${LOGNAME}/fv3sar_workflow
 export FIXgsm=$BASE_GSM/fix/fix_am
 export DATA=/scratch4/NCEPDEV/stmp4/${LOGNAME}/wrk.chgres
 export APRUNC="time"
 export ymd=`echo $CDATE | cut -c 1-8`
 if [ $ictype = pfv3gfs ]; then
  hour=`echo $CDATE | cut -c 9-10`
  export INIDIR=/scratch4/NCEPDEV/fv3-cam/noscrub/Eric.Rogers/prfv3rt1/gfs.$ymd/$hour
 else
  export COMROOTp2=/scratch4/NCEPDEV/rstprod/com
  export INIDIR=$COMROOTp2/gfs/prod/gfs.$ymd
 fi
 export HOMEgfs=$PBS_O_WORKDIR/..
 export HOMEgfs=$SLURM_SUBMIT_DIR/..
 ulimit -a
 ulimit -s unlimited
else
 echo "$machine not supported, exit"
 exit
fi
#
# remove working directory to make sure no previous runs are left behind
# it will be created in the global_chgres_driver.sh script
#
if [ -d $DATA ]; then rm -rf $DATA; fi
export FIXfv3=$BASE_GSM/fix/fix_fv3
export CDAS=gfs                  # gfs or gdas
CRES=`echo $CASE | cut -c 2-`
export OUTDIR=$FIXfv3/C${CRES}
#
# if this is generating data for a regional run,
# now create the boundary data except for the initial data since
# that was created in the earlier call above
#
# start at hour 3 and create BC's every 3 hours
#
#
# set the links to use the 4 halo grid and orog files
# these are necessary for creating the boundary data
#
 ln -sf $FIXfv3/$CASE/${CASE}_grid.tile7.halo4.nc $FIXfv3/$CASE/${CASE}_grid.tile7.nc
 ln -sf $FIXfv3/$CASE/${CASE}_oro_data.tile7.halo4.nc $FIXfv3/$CASE/${CASE}_oro_data.tile7.nc
#
hour=3
end_hour=24
bc_interval=3
while (test "$hour" -le "$end_hour")
 do
  if [ $hour -lt 10 ]; then
   hour_name='00'$hour
  elif [ $hour -lt 100 ]; then
   hour_name='0'$hour
  else
   hour_name=$hour
  fi
 if [ $machine = WCOSS_C ]; then
#
#create input file for cfp in order to run multiple copies of global_chgres_driver.sh simultaneously
#since we are going to run simulataneously, we want different working directories for each hour
#
  BC_DATA=/gpfs/hps3/ptmp/${LOGNAME}/wrk.chgres.$hour_name
  echo "env REGIONAL=2 HALO=4 bchour=$hour_name DATA=$BC_DATA $BASE_GSM/ush/global_chgres_driver.sh >&out.chgres.$hour_name" >>bcfile.input
 elif [ $machine = WCOSS ]; then
  BC_DATA=/ptmpp2/${LOGNAME}/wrk.chgres.$hour_name
  echo "env REGIONAL=2 HALO=4 bchour=$hour_name DATA=$BC_DATA $BASE_GSM/ush/global_chgres_driver.sh >&out.chgres.$hour_name" >>bcfile.input
 elif [ $machine = DELL ]; then
  BC_DATA=/gpfs/dell3/ptmp/${LOGNAME}/wrk.chgres.$hour_name
  echo "env REGIONAL=2 HALO=4 bchour=$hour_name DATA=$BC_DATA $BASE_GSM/ush/global_chgres_driver.sh >&out.chgres.$hour_name" >>bcfile.input
 elif [ $machine = THEIA ]; then
#
#for now on theia run the BC creation sequentially
#
  export REGIONAL=2
  export HALO=4
  export bchour=$hour_name
  $BASE_GSM/ush/global_chgres_driver.sh
 fi
  hour=`expr $hour + $bc_interval`
done
#
# for WCOSS systems we now run BC creation for all hours simultaneously
#
 if [ $machine = WCOSS_C ]; then
  export APRUNC=time
  export OMP_NUM_THREADS_CH=24      #default for openMP threads
  aprun -j 1 -n 28 -N 1 -d 24 -cc depth cfp bcfile.input
  rm bcfile.input
 elif [ $machine = WCOSS ]; then
  export OMP_NUM_THREADS_CH=24      #default for openMP threads
  export MP_CSS_INTERRUPT=yes
  mpirun.lsf cfp bcfile.input
  rm bcfile.input
 elif [ $machine = DELL ]; then
  export OMP_NUM_THREADS_CH=24      #default for openMP threads
  mpirun cfp bcfile.input
  rm bcfile.input
 fi
#
#remove the links that were set above for the halo4 files
#
if [ $gtype = regional ] ; then
 rm $FIXfv3/$CASE/${CASE}_grid.tile7.nc
 rm  $FIXfv3/$CASE/${CASE}_oro_data.tile7.nc
fi
exit
